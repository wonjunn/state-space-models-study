# state-space-models-study

## Papers

<details><summary> <strong> HiPPO: Recurrent Memory with Optimal Polynomial Projections </strong> <code>NeurIPS 2020</code> <br> <a href="https://arxiv.org/abs/2008.07669"><img alt="GitHub release" src="https://img.shields.io/badge/arXiv-2008.07669-b31b1b.svg?style=flat-square"></a> </summary>
  
![image](https://github.com/wonjunn/state-space-models-study/assets/60861873/3bbe2dd0-b97f-4456-92fa-d6a8e3e1131b)

</details>

<details><summary> <strong> Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers </strong> <code>NeurIPS 2021</code> <br> <a href="https://arxiv.org/abs/2110.13985"><img alt="GitHub release" src="https://img.shields.io/badge/arXiv-2110.13985-b31b1b.svg?style=flat-square"></a> </summary>
  
![image](https://github.com/wonjunn/state-space-models-study/assets/60861873/ddad1821-6727-45d7-8132-982ba5aad42f)

</details>

<details><summary> <strong> Efficiently Modeling Long Sequences with Structured State Spaces </strong> <code>ICLR 2022</code> <br> <a href="https://arxiv.org/abs/2111.00396"><img alt="GitHub release" src="https://img.shields.io/badge/arXiv-2111.00396-b31b1b.svg?style=flat-square"></a> </summary>
  
![image](https://github.com/wonjunn/state-space-models-study/assets/60861873/7196a443-8ab2-4620-b0ad-164bd1c2db9d)

</details>

<details><summary> <strong> On the Parameterization and Initialization of Diagonal State Space Models </strong> <code>NeurIPS 2023</code> <br> <a href="https://arxiv.org/abs/2206.11893"><img alt="GitHub release" src="https://img.shields.io/badge/arXiv-2206.11893-b31b1b.svg?style=flat-square"></a> </summary>


</details>

</details>

<details><summary> <strong> Hungry Hungry Hippos: Towards Language Modeling with State Space Models </strong> <code>ICLR 2023</code> <br> <a href="https://arxiv.org/abs/2212.14052"><img alt="GitHub release" src="https://img.shields.io/badge/arXiv-2212.14052-b31b1b.svg?style=flat-square"></a> </summary>


</details>

<details><summary> <strong> Mamba: Linear-Time Sequence Modeling with Selective State Spaces </strong> <code>Preprint</code> <br> <a href="https://arxiv.org/abs/2312.00752"><img alt="GitHub release" src="https://img.shields.io/badge/arXiv-2312.00752-b31b1b.svg?style=flat-square"></a> </summary>


</details>

## Useful Links

<strong> A Visual Guide to Mamba and State Space Models </strong> <br>
<a href="https://open.substack.com/pub/maartengrootendorst/p/a-visual-guide-to-mamba-and-state?utm_campaign=post&utm_medium=web"><img alt="GitHub release" src="https://img.shields.io/badge/Substack-FF6719?logo=substack&logoColor=fff&style=flat-square"></a>

<strong> Do we need Attention? - Linear RNNs and State Space Models (SSMs) for NLP </strong> <br>
<a href="https://youtu.be/dKJEpOtVgXc?si=kvYkRG6gGVwuBDnC"><img alt="GitHub release" src="https://img.shields.io/badge/YouTube-F00?logo=youtube&logoColor=fff&style=flat-square"></a>

<strong> The Annotated S4 </strong> <br>
<a href="https://srush.github.io/annotated-s4/"><img alt="GitHub release" src="https://img.shields.io/badge/GitHub%20Pages-222?logo=githubpages&logoColor=fff&style=flat-square"></a>
