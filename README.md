# Resources for State Space Models (SSM)

## Must-Read Papers

<details><summary> <strong> HiPPO: Recurrent Memory with Optimal Polynomial Projections </strong> <code>NeurIPS 2020</code> <code>#HiPPO</code> <br> <a href="https://arxiv.org/abs/2008.07669"><img src="https://img.shields.io/badge/arXiv-2008.07669-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/HazyResearch/hippo-code"><img src="https://img.shields.io/github/stars/HazyResearch/hippo-code?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> <a href="https://hazyresearch.stanford.edu/blog/2020-12-05-hippo">
  <img src="https://github.com/wonjunn/state-space-models-study/assets/60861873/6746910e-7ed0-43bb-9bd7-2d8c3ef52964" width="20" />
</a> </summary>
  
![image](https://github.com/wonjunn/state-space-models-study/assets/60861873/3bbe2dd0-b97f-4456-92fa-d6a8e3e1131b)

</details>

<details><summary> <strong> Efficiently Modeling Long Sequences with Structured State Spaces </strong> <code>ICLR 2022</code> <code>#S4</code> <br> <a href="https://arxiv.org/abs/2111.00396"><img alt="GitHub release" src="https://img.shields.io/badge/arXiv-2111.00396-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/state-spaces/s4"><img src="https://img.shields.io/github/stars/state-spaces/s4?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> <a href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-1">
  <img src="https://github.com/wonjunn/state-space-models-study/assets/60861873/6746910e-7ed0-43bb-9bd7-2d8c3ef52964" width="20" />
</a> </summary>
  
![image](https://github.com/wonjunn/state-space-models-study/assets/60861873/7196a443-8ab2-4620-b0ad-164bd1c2db9d)

</details>

<details><summary> <strong> Hungry Hungry Hippos: Towards Language Modeling with State Space Models </strong> <code>ICLR 2023</code> <code>#H3</code> <br> <a href="https://arxiv.org/abs/2212.14052"><img alt="GitHub release" src="https://img.shields.io/badge/arXiv-2212.14052-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/HazyResearch/H3"><img src="https://img.shields.io/github/stars/HazyResearch/H3?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> <a href="https://hazyresearch.stanford.edu/blog/2023-01-20-h3">
  <img src="https://github.com/wonjunn/state-space-models-study/assets/60861873/6746910e-7ed0-43bb-9bd7-2d8c3ef52964" width="20" />
</a> </summary>

![image](https://github.com/wonjunn/state-space-models-study/assets/60861873/6c080966-b0f9-408f-a4c6-c5e1fbd2ba44)

</details>

<details><summary> <strong> Mamba: Linear-Time Sequence Modeling with Selective State Spaces </strong> <code>Preprint</code> <code>#S6</code> <code>#Mamba</code> <br> <a href="https://arxiv.org/abs/2312.00752"><img src="https://img.shields.io/badge/arXiv-2312.00752-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/state-spaces/mamba"><img src="https://img.shields.io/github/stars/state-spaces/mamba?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

![image](https://github.com/wonjunn/state-space-models-study/assets/60861873/74d75d5c-b675-4120-9391-f90203257578)

</details>

## State Matrix $A$

### HiPPO Matrix

<details><summary> <strong> Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers </strong> <code>NeurIPS 2021</code> <code>#LSSL</code> <br> <a href="https://arxiv.org/abs/2110.13985"><img src="https://img.shields.io/badge/arXiv-2110.13985-b31b1b.svg?style=flat-square"></a> </summary>
  
![image](https://github.com/wonjunn/state-space-models-study/assets/60861873/ddad1821-6727-45d7-8132-982ba5aad42f)

</details>

</details>

<details><summary> <strong> How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections </strong> <code>ICLR 2023</code> <br> <a href="https://arxiv.org/abs/2206.12037"><img src="https://img.shields.io/badge/arXiv-2206.12037-b31b1b.svg?style=flat-square"></a> </summary>

</details>

### Structured Matrix

<details><summary> <strong> Efficiently Modeling Long Sequences with Structured State Spaces </strong> <code>ICLR 2022</code> <code>#S4</code> <br> <a href="https://arxiv.org/abs/2111.00396"><img alt="GitHub release" src="https://img.shields.io/badge/arXiv-2111.00396-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/state-spaces/s4"><img src="https://img.shields.io/github/stars/state-spaces/s4?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> <a href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-1">
  <img src="https://github.com/wonjunn/state-space-models-study/assets/60861873/6746910e-7ed0-43bb-9bd7-2d8c3ef52964" width="20" />
</a> </summary>
  
![image](https://github.com/wonjunn/state-space-models-study/assets/60861873/7196a443-8ab2-4620-b0ad-164bd1c2db9d)

</details>

### Diagonal Matrix

<details><summary> <strong> Diagonal State Spaces are as Effective as Structured State Spaces </strong> <code>NeurIPS 2022</code> <code>#DSS</code> <br> <a href="https://arxiv.org/abs/2203.14343"><img src="https://img.shields.io/badge/arXiv-2203.14343-b31b1b.svg?style=flat-square"></a> </summary>

</details>

</details>

<details><summary> <strong> On the Parameterization and Initialization of Diagonal State Space Models </strong> <code>NeurIPS 2023</code> <code>#S4D</code> <br> <a href="https://arxiv.org/abs/2206.11893"><img src="https://img.shields.io/badge/arXiv-2206.11893-b31b1b.svg?style=flat-square"></a> </summary>

![image](https://github.com/wonjunn/state-space-models-study/assets/60861873/efa33a0a-f487-409a-b251-1292e1f4ade9)

</details>

## Linear Attention & RNN

<details><summary> <strong> Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention </strong> <code>ICML 2020</code> <br> <a href="https://arxiv.org/abs/2006.16236"><img src="https://img.shields.io/badge/arXiv-2006.16236-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/idiap/fast-transformers"><img src="https://img.shields.io/github/stars/idiap/fast-transformers?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

<details><summary> <strong> Resurrecting Recurrent Neural Networks for Long Sequences </strong> <code>ICML 2023</code> <code>#LRU</code> <br> <a href="https://arxiv.org/abs/2303.06349"><img src="https://img.shields.io/badge/arXiv-2303.06349-b31b1b.svg?style=flat-square"></a> </summary>

</details>

<details><summary> <strong> RWKV: Reinventing RNNs for the Transformer Era </strong> <code>EMNLP 2023</code> <code>#RWKV</code> <br> <a href="https://arxiv.org/abs/2305.13048"><img src="https://img.shields.io/badge/arXiv-2305.13048-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/BlinkDL/RWKV-LM"><img src="https://img.shields.io/github/stars/BlinkDL/RWKV-LM?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

<details><summary> <strong> Simple linear attention language models balance the recall-throughput tradeoff </strong> <code>Preprint</code> <code>#BASED</code> <br> <a href="https://arxiv.org/abs/2402.18668"><img src="https://img.shields.io/badge/arXiv-2402.18668-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/HazyResearch/based"><img src="https://img.shields.io/github/stars/HazyResearch/based?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

<details><summary> <strong> Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models </strong> <code>Preprint</code> <code>#Griffin</code> <code>#Hawk</code> <br> <a href="https://arxiv.org/abs/2402.19427"><img src="https://img.shields.io/badge/arXiv-2402.19427-b31b1b.svg?style=flat-square"></a> </summary>

</details>

## Analysis

<details><summary> <strong> The Hidden Attention of Mamba Models </strong> <code>Preprint</code> <br> <a href="https://arxiv.org/abs/2403.01590"><img src="https://img.shields.io/badge/arXiv-2403.01590-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/AmeenAli/HiddenMambaAttn"><img src="https://img.shields.io/github/stars/AmeenAli/HiddenMambaAttn?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>


## Evaluation

<details><summary> <strong> Long Range Arena: A Benchmark for Efficient Transformers </strong> <code>ICLR 2021</code> <br> <a href="https://arxiv.org/abs/2011.04006"><img src="https://img.shields.io/badge/arXiv-2011.04006-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/google-research/long-range-arena"><img src="https://img.shields.io/github/stars/google-research/long-range-arena?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

<details><summary> <strong> Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors </strong> <code>ICLR 2024</code> <br> <a href="https://arxiv.org/abs/2310.02980"><img src="https://img.shields.io/badge/arXiv-2310.02980-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/IdoAmos/not-from-scratch"><img src="https://img.shields.io/github/stars/IdoAmos/not-from-scratch?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

<details><summary> <strong> Zoology: Measuring and Improving Recall in Efficient Language Models </strong> <code>ICLR 2024</code> <br> <a href="https://arxiv.org/abs/2312.04927"><img src="https://img.shields.io/badge/arXiv-2312.04927-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/HazyResearch/zoology"><img src="https://img.shields.io/github/stars/HazyResearch/zoology?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

## Applications

### Vision

<details><summary> <strong> Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model </strong> <code>Preprint</code> <code>#Vim</code> <br> <a href="https://arxiv.org/abs/2401.09417"><img src="https://img.shields.io/badge/arXiv-2401.09417-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/hustvl/Vim"><img src="https://img.shields.io/github/stars/hustvl/Vim?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

<details><summary> <strong> VMamba: Visual State Space Model </strong> <code>Preprint</code> <code>#VMamba</code> <br> <a href="https://arxiv.org/abs/2401.10166"><img src="https://img.shields.io/badge/arXiv-2401.10166-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/MzeroMiko/VMamba"><img src="https://img.shields.io/github/stars/MzeroMiko/VMamba?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

## Survey

<strong> On the Resurgence of Recurrent Models for Long Sequences -- Survey and Research Opportunities in the Transformer Era </strong> <br>
<a href="https://arxiv.org/abs/2402.08132"><img src="https://img.shields.io/badge/arXiv-2402.08132-b31b1b.svg?style=flat-square"></a>

## Useful Links

<strong> Hazy Research Blog </strong> <br>
<a href="https://hazyresearch.stanford.edu/blog">
  <img src="https://github.com/wonjunn/state-space-models-study/assets/60861873/6746910e-7ed0-43bb-9bd7-2d8c3ef52964" width="20" />
</a>

<strong> A Visual Guide to Mamba and State Space Models </strong> <code>#Mamba</code> <br>
<a href="https://open.substack.com/pub/maartengrootendorst/p/a-visual-guide-to-mamba-and-state?utm_campaign=post&utm_medium=web"><img src="https://img.shields.io/badge/Substack-FF6719?logo=substack&logoColor=fff&style=flat-square"></a>

<strong> Do we need Attention? - Linear RNNs and State Space Models (SSMs) for NLP </strong> <br>
<a href="https://youtu.be/dKJEpOtVgXc?si=kvYkRG6gGVwuBDnC"><img src="https://img.shields.io/badge/YouTube-F00?logo=youtube&logoColor=fff&style=flat-square"></a> <a href="https://github.com/srush/do-we-need-attention"><img src="https://img.shields.io/github/stars/srush/do-we-need-attention?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a>

<strong> Mamba and S4 Explained: Architecture, Parallel Scan, Kernel Fusion, Recurrent, Convolution, Math </strong> <br>
<a href="https://youtu.be/8Q_tqwpTpVU?si=jWkH3QfOgRNDqURi"><img src="https://img.shields.io/badge/YouTube-F00?logo=youtube&logoColor=fff&style=flat-square"></a> <a href="https://github.com/hkproj/mamba-notes"><img src="https://img.shields.io/github/stars/hkproj/mamba-notes?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a>

<strong> The Annotated S4 </strong> <code>#S4</code> <br>
<a href="https://srush.github.io/annotated-s4/"><img src="https://img.shields.io/badge/GitHub%20Pages-222?logo=githubpages&logoColor=fff&style=flat-square"></a>

<strong> Mamba: The Hard Way </strong> <code>#Mamba</code> <br>
<a href="https://srush.github.io/annotated-mamba/hard.html"><img src="https://img.shields.io/badge/GitHub%20Pages-222?logo=githubpages&logoColor=fff&style=flat-square"></a>

<strong> RNNs strike back </strong> <br>
<a href="https://adrian-valente.github.io/2023/10/03/linear-rnns.html"><img src="https://img.shields.io/badge/GitHub%20Pages-222?logo=githubpages&logoColor=fff&style=flat-square"></a>

<strong> Mamba No. 5 (A Little Bit Of...) </strong> <code>#Mamba</code> <br>
<a href="https://jameschen.io/jekyll/update/2024/02/12/mamba.html"><img src="https://img.shields.io/badge/GitHub%20Pages-222?logo=githubpages&logoColor=fff&style=flat-square"></a>
